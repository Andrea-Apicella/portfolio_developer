<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet" />
		<link rel="stylesheet" href="../style.css" />
		<title>Fall Detection Research</title>
		<script src="https://kit.fontawesome.com/d69139bf03.js" crossorigin="anonymous"></script>
	</head>
	<body>
		<div class="go-up-icon" onclick="goUp()">
			<i class="fas fa-angle-up fa-2x"></i>
		</div>
		<header>
			<nav class="navbar navbar-blog">
				<a href="../index.html">
					<div class="brand-title">
						<h1>A</h1>
					</div>
				</a>
				<div id="toggle-button">
					<span id="top-bar"></span>
					<span id="mid-bar"></span>
					<span id="bot-bar"></span>
				</div>
				<div class="navbar-links">
					<ul>
						<li class="nav-item"><a href="../index.html" class="nav-link active">Home</a></li>
						<li class="nav-item"><a href="../index.html#about" class="nav-link">About</a></li>
						<li class="nav-item"><a href="../index.html#research" class="nav-link">Research</a></li>
						<li class="nav-item"><a href="../index.html#education" class="nav-link">Education</a></li>
						<li class="nav-item"><a href="../index.html#past-works" class="nav-link">Past Works</a></li>
						<li class="nav-item"><a href="#contact" class="nav-link">Contact</a></li>
					</ul>
				</div>
			</nav>
		</header>
		<main id="fall-detection" class="blog-content">
			<section class="banner">
				<div>
					<h1 class="banner-title" data-aos="fade-up" data-aos-duration="1000">Fall Detection Research</h1>
				</div>
			</section>
			<section class="section">
				<div class="blog-image" data-aos="fade-up" data-aos-duration="800" data-aos-offset="-200">
					<img src="../images/research/fall-detection/fall-detection-blog-image.webp" alt="cover image for fall detection page" />
					<p class="image-caption">March 2020 - January 2021</p>
				</div>
				<div class="sub-section">
					<h1 class="section-title" data-aos="fade-up" data-aos-duration="800" data-aos-offset="200">Real-time remote fall detection using Deep Neural Networks</h1>
					<p class="section-p" data-aos="fade-up" data-aos-duration="800">This is my first research project using Machine Learning. I developed a fall detection system based on Deep Neural Networks which runs inside the browser in real time. This project, which originally started as my bachelor's thesis, has been accepted for pubblication by <a href="http://phuselab.di.unimi.it/CARE2020/">CARE2020</a> workshop held during <a href="https://www.icpr2020.it">ICPR2020</a> conference. Below you will find the video presentation made for the conference and the written explanation of how the system works, or you can directly read or download the published paper using the buttons below.</p>
					<!-- <button class="button">Download Paper</button> -->
					<a href="https://link.springer.com/chapter/10.1007/978-3-030-68790-8_16"><button class="button">View Publication</button></a>
				</div>
			</section>
			<section id="ICPR-video-container" class="section">
				<div class="sub-section" data-aos="fade-up" data-aos-duration="800" data-aos-offset="200">
					<h2 class="section-subtitle">Video presentation</h2>
					<div class="youtube-wrapper">
						<iframe id="ICPR-video" width="100%" height="auto" src="https://www.youtube.com/embed/t7PABimbN5s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
					</div>
				</div>
			</section>
			<section class="section">
				<div class="sub-section" data-aos="fade-up" data-aos-duration="800">
					<h2 class="section-subtitle">How does it work</h2>
					<p class="section-p">Falls are one the most prominent cause of accidental injury deaths worldwide. In 2018, as reported by the World Health Organization, only car accidents caused more deaths than falls did. The most affected ones by this type of accident, as can be expected, are the elderly and in particular, adults older than 65 years old suffer the greatest number of fatal falls. Being falls such a common and at the same time dangerous kind of accident, the problem of fall detection received much interest from researchers during the last years. Recently, deep learning proved to an optimal solution to tackle such problem. Many pieces of research feature deep neural network architectures and many different types of sensors to address the problem, including cameras, accelerometers, Wi-FI and a mix of the previous. Although there is no one type of sensor generally more suitable than others to perceive a fall, we chose to use only vision based sensors.</p>
					<img src="../images/research/fall-detection/proposed_approach.webp" width="100%" height="auto" alt="image showing the key features of the proposed approach" />
					<p class="section-p">This choice came from our will to develop a fall detection system which to be used would not require to purchase additional hardware, or software. With our architecture running inside the web browser, one could use even a smartphone or a device with an integrated camera as an advanced fall detection tool. In particular, we used videos from RGB cameras only. We chose not to use depth cameras as they aren't as widespread as the RGB ones. As long as the camera can be connected to a computer and work like a webcam, our method requires to have only a web browser installed. We are able to process the videos inside the browser as our neural networks are converted to TensorFlow.js after training. With TensorFlow.js the performance is not as good as with Tensorflow, but the results we obtained clearly show that it is already possible to run Deep Neurale Networks in real time even inside the web browser and even on low powered devices. Let's take a look inside our architecture.</p>
					<img src="../images/research/fall-detection/architecture overview.webp" width="100%" height="auto" alt="image showing the fall detection archicture main steps" />
					<p class="section-p">We employ a pose estimation model named <a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet">PoseNet</a> to extract a pose from each video frame. If PoseNet's prediction is not accurate enough a supplementary Convolutional Neural Network comes into play, performing a furter prediction. Once extracted, the poses are fed to a LSTM network to perform binay classification. Thereby we can distinguish the series of poses which contain a fall by the ones which don't.</p>
					<p class="section-p">The frames we process come from two of the fall detection dataset which are used the most in literature. The first one is UR Fall detection dataset, wich contains the raw videos acquired with two rgb cameras as well as depth and accelerometer data. We only use rgb videos from the first camera wich is placed parallel to the ground. The second dataset is called Multicam and features 8 synchronized RGB cameras. This dataset contains much more frames the the first but the subject often takes a very small portion of the frame which sometimes causes problems with the pose estimation model we used. In order to use frames from both URFall and Multicam we merged the two dataset and split the resulting one in two parts. We used one of these parts for training the supplementary CNN and the other to perform poses extraction.</p>
					<div class="inline-desktop">
						<img class="img-inline" src="../images/research/fall-detection/multicam.webp" width="100%" height="auto" alt="image showing Multicam dataset features" />
						<img class="img-inline" src="../images/research/fall-detection/urfall.webp" width="100%" height="auto" alt="image showing URFall dataset features" />
					</div>
					<p class="section-p">The fall detection model we used is able to detect the 2d coordinates of 17 human body keypoints and it's implemented in tensorflow.js to run smoothly in the web browser. Along with the 34 numerical values corresponding to the x and y coordinates of each keypoint, PoseNet returns a confidence score for each keypoint location and an overall confidence score which refers to the whole pose.</p>
					<img src="https://1.bp.blogspot.com/-yRn-UAqO8Mc/XcCkQ6nKOrI/AAAAAAAAAww/H4FvoPHdg-oQJJ_-03qK5lsXXGEFRJoWwCLcBGAsYHQ/s320/p1.gif" width="100%" height="auto" alt="posenet demo image" />
					<p class="section-p">When testing PoseNet we discovered that it struggles with detecting the pose of subjects who are lying on the ground. The kind of situation is obviously predominant in our datasets, so to address this problem we implemented a supplementary Convolutional Neural Network to reinforce the pose extraction phase.</p>
					<img src="../images/research/fall-detection/posenet_fails.webp" width="100%" height="auto" alt="images showing how PoseNet fails to properly detect all the keypoints of a subject who is lying down" />
					<p class="section-p">The CNN we chose is MobileNetV2. This CNN architecture was developed to be usable even on mobile devices while still achieving decent accuracy compared to heavier CNNs. For our work we employed a pre-trained and replaced the top layer with custom ones to fine tune the model on our dataset. In particular we trained the model to distinguish the frames which contain a person who is lying down and the ones which do not.</p>
					<img src="../images/research/fall-detection/mobilenetv2.webp" width="100%" height="auto" alt="image showing the MobileNetV2 CNN classification task" />
					<p class="section-p">To understand why, you need to take a look at the poses extraction framework we developed. As i said we extract one pose from each video frame, and we do this processing completely inside the web browser.</p>
					<video muted playsinline autoplay loop width="100%" height="auto" type="video/mp4" src="../images/research/fall-detection/poses-extraction.mp4"></video>

					<p class="section-p">Firstly, PoseNet tries to estimate the position of the 17 body keypoints. If no pose is found, or if the confidence score of more than two thirds of the keypoints is below 30%, we run the supplementary CNN. Based on the prediction of the CNN, we choose to save the original pose estimated by PoseNet, an empty pose or a dummy pose. This dummy pose is a pose we previously extracted from a video frame which contained a lying down subject, but of whom PoseNet was able to correctly identify all of the 17 keypoints. With this method we are able assing some information even to those frames from which we wouldn't be extract any if we just relied on PoseNet.</p>
					<img src="../images/research/fall-detection/poses_extraction_architecture.webp" width="100%" height="auto" alt="image showing the full poses extraction architecture" />
					<p class="section-p">Once we have one pose for each video frame, we split such poses in time series of 20 and feed thos series to a LSTM network. The network is composed of 2 LSTM layers with 34 neurons each. We add dropout after each LSTM layer to prevent overfitting and we have a final Dense layer with sigmoid activation to perform binary classification.</p>
					<img src="../images/research/fall-detection/lstm.webp" width="100%" height="auto" alt="" />

					<p class="section-p">Training the LSTM with an 80% training and 20% validation split we achieved 82% average accuracy. At the same time we outperform in terms of latency various state of the art fall detection methods which, unlike ours, run offline and which use much more powerful GPUs than we do.</p>

					<img src="../images/research/fall-detection/results.webp" width="100%" height="auto" alt="table showing the experimental results we obtained" />

					<p class="section-p">To recap, our solution is completely web based, and so can be broadly distributed as a web app. The only requirements are a web browser and a RGB camera. Users privacy is granted as the processing is done completely offline and there is no need to send the data to a server and we can achieve real time performance even on low-powered machines with integrated GPUs. Finally, we are planning to further improve this architecture by learning the task end to end, without relying on poses and implementing some visual attention mechanism to improve the accuracy of our architecture.</p>
					<img src="../images/research/fall-detection/recap.webp" width="100%" height="auto" alt="image showing the recap key points of my architecture" />
				</div>
			</section>
		</main>
		<footer>
			<section class="section" id="contact">
				<h1 data-aos="fade-up" data-aos-delay="0" data-aos-duration="800" class="section-title">Contact Me</h1>
				<p data-aos="fade-up" data-aos-delay="100" data-aos-duration="800" class="section-p">I check most frequently whatsapp telegram and email, but feel free to get in touch on the platform you prefer!</p>
				<div data-aos="fade-up" data-aos-delay="200" data-aos-offset="-200" data-aos-duration="800" class="icons-container">
					<div class="icon">
						<a href="https://wa.me/393312116912"> <i class="font-awesome-icon fab fa-whatsapp"></i></a>
					</div>
					<a href="https://t.me/andreaapi">
						<div class="icon">
							<i class="font-awesome-icon fab fa-telegram-plane"></i></div
					></a>
					<a href="mailto:andrea.apicella221@gmail.com">
						<div class="icon">
							<i class="font-awesome-icon fas fa-envelope"></i></div
					></a>
					<a href="https://github.com/Andrea-Apicella">
						<div class="icon">
							<i class="font-awesome-icon fab fa-github"></i></div
					></a>
					<a href="https://www.instagram.com/andreaapicella_/">
						<div class="icon">
							<i class="font-awesome-icon fab fa-instagram"></i>
						</div>
					</a>
				</div>
			</section>
			<section class="credits">
				<p>This website is an ongoing project. Check out how i developed it on my <a href="https://github.com/Andrea-Apicella/portfolio_developer">Github</a></p>
			</section>
		</footer>

		<script src="../js/smooth-scroll.min.js"></script>
		<script src="../js/smooth-scroll.polyfills.min.js"></script>
		<script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
		<script>
			const scroll = new SmoothScroll('a[href*="#"]', {
				speed: 400,
				offset: 30,
				easing: 'easeOutQuint',
			});

			AOS.init();
		</script>
		<script src="../js/main.js"></script>
	</body>
</html>
